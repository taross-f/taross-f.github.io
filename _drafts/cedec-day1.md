## 『ソードアート・オンライン』 仮想から現実へ。 小説とゲーム技術のお話。 ～ソードアート・オンラインが現実になる日まで。～

注視したら情報密度が上がる、など
神経全部つながったらもはやエンターテイメントではない


なぜゲーム化したのか

仮想空間計画

ナーブギア 強引なしかけだった
要件から考えて設定を決めた

VR装着のめんどくささ、テクノロジ待ち、ハード

## 中規模チームを支える自動化と属人化しないノウハウ共有の取り組み

https://speakerdeck.com/konboi/cedec2017


アチーブメントシート

QA
アイテム追加時のID重複は?
=> IDは予め決めてからマスタに書く、 + スクリプトでチェック

アチーブメントシートの項目は
=> チームのエンジニア全体で話あってきめた + 適度に見直しを全体で行う

dockerの後処理
=> バッチで落としている、ブランチが生きてるかとかみる

スキルマップをいやがらないか
=> 評価には絶対反映させない

## クラウド時代の長く効率よく運用するためのゲームサーバとインフラ設計

https://goo.gl/AKnR4H
https://drive.google.com/file/d/0B0dJLHELD8SJQVhaVDZCbXJUblk/view

ID % 100 で水平分割


## 複雑化するAssetBundleの配信からロードまでを基盤化した話

AB配信基盤Octo

### 基盤化の経緯
ネイティブシフト

ノウハウがない状態でリリース

CPUメモリ制限
通信環境が特殊
多様な端末
OSのUpdate
アプリサイズ制限

#### AB とは
アセットをまとめたもの
アプリ容量制限を回避

#### AB問題
* 管理
  * 世代管理が困難
  * 量が増えるので管理が大変
* クライアント
  * 同一ABは複数同時ロードできない
  * キャッシュ済みABを削除する手段がない

#### いままで
SVN=>jenkinsでビルド=> 配置

#### 実現イメージ
配置するサーバーが配信専用サーバー



### 要件定義
サーバー
* 運用が楽
* 配信が早い
* 正確
SDK
* 動作が高速
* 簡単


### 基本設計
サーバー
* サーバーの管理をしない
* 適切なキャッシュ
* わかりやすいロジック
SDK
* キャッシュ管理を適切に
* 通信速度
* UnityAPIに似せる


### サーバー設計
配信基盤サーバー
* DB動機API
* URL取得API
* アップロードAPI
* 削除API

問題点
* データ量 ふくらむ
* 突発的なトラフィック
* 大量データのやりとり
* ABの世代管理

3Dゲーム  10~数百GB

GoogleCloudStorageを使用する
* スケーラビリティ
* MultiRegional


トラフィックに対して
GCP GKE

運用コスト 基盤は一度できたらそこまで頻繁に更新されないのでほっておける
* ハードウェア管理
* セキュリティ
社外からのアクセスが用意
* VPN

docker
* 運用を楽にしたい
  * 構成管理不要
  * 技術の習得コスト
* シンプル
  * DockerFileのみ
* GKEがある

GKE
* クラスタ管理
* 監視  StackDriverとの連携
* ログ  CloudLogging
* OSS 豊富な情報、活発なコミュニティ


大量のデータのやりとり
* AB
* DBの同期
revisionでキャッシュ
ETAGでキャッシュコントロール


世代管理について
* リリースごとの差分がとれる
* 端末ごとの差分の管理
* AB更新の管理

全体のリビジョン | ファイルのリビジョン | 端末のリビジョン

0   1   2
    1   1
    1   1
        2

更新･追加時に全体リビジョンをあげる

端末のリビジョンで差分のあるファイルを取得する


* GCSでスケーラブル
* GCP GKE 
* CDNをつかってABとDBの差分を高速に配信

### 運用機能
* CLI
AB Upload
Tag
差分確認

jenkinsが触る

* 管理画面
リビジョン情報などが取れる

copy/sync
copy:の場合はリビジョンを振り直す
sync:リビジョンもそのままコピーする

### UnitySDK
#### クライアント課題
自由度が低い

キャッシュ削除ができない

通信のハンドリング

→
* Unityコンポーネントの置き換え 自由度工場
* 置き換え出来ない場合はラップ

 

* 必要な機能
ABメタデータ世代管理
ABキャッシュの独自管理

ABロード管理(ラップ)



* 開発ポリシー
  * パフォーマンス 大事
  * どんなゲームでも使える自由度
    * 汎用的実用的API
  * 低い導入ハードル
    * UnityAPIの名前互換性
    * 外部の依存を極力減らす


#### 初期化
* ローカルDBロード 更新
* 端末内キャッシュ操作

ローカルDBとは
* Octo独自情報
  * ID タグなど
* AB

ProtocolBufferで配列のままシリアライズし保存
AESで暗号化
差分更新
  * レコード単位のち感
  * 通信量の削減


ローカルDB 検索インデックスはメモリ消費する
* ローカルDBのレコード数=AB数
* 1万件超えは普通

byte[] 参照のためのアドレス分
配列のオーバーヘッド
メモリアライメントのパディング
->
structで力技で1byteずつ展開


キャッシュ操作について
独自のキャッシュ管理

当時のUnityキャッシュ問題
* キャッシュ増えると起動時間ながくなる
* 使ってないキャッシュの自動削除が150日以上
* 異なるバージョンの同盟キャッシュを別に保存する
* 個別削除APIがない

独自キャッシュ管理
* 初期化と同時にキャッシュ操作を別スレッドで始める
* 自動削除しないオプション用意
* キャッシュの管理単位を名前だけで行う
   * 同名だと複数保存されない

#### ABロードについて
参照数で自動開放
ロード要求 - アンロード要求 
refcountっぽい

GOのDestroyでも参照増減


Resource機能
ABでなくそのままファイルを配信できる


#### 通信について
いまや必需品

* 通信タイムアウトの問題
大きいファイルを低速な通信環境でDL完了できなくなる

タイムアウトは無通信が続いた時間で判定
毎フレーム受信サイズ見る
10秒変わらなかったらタイムアウト


#### 通信APIについて
* HttpWebRequest
  * 自由度高い
  * HTTPS通信ができない TLS証明書の検証がサポートされていない
* WWW
  * 設計が古く機能も拡張性もとぼしい
* UnityWebRequest
  * ワンソースでマルチプラットフォーム対応
  * 自由度はそれない
  * 拡張用仕組みあり
  * Native領域でフルでバッファリングしなくてはいけない、
    * DownloadHandlerScriptを使ってデータ受信のカスタム処理
       * 固定長バッファ
    * C#へのメモリコピーコストがきつい
    * DownloadHandlerがメインスレッドで動く
* ネイティブプラグイン
  * C#へのバッファのコピーがなくなる
    * 時間短縮 GC避ける
  * 寄り効率的なパイプライン化が可能
  * OSのサポートをフルで受けられる
     * 2017.2で本家で入る

ネイティブプラグイン (ファイルに保存しない場合はUnityWebRequest)

#### まとめ
* 技術チャレンジ
* 端末ローカル管理 柔軟な世代管理 ゲームの自由度工場
* 通信 ネイティブプラグインで効率的にDLできた

### まとめ
1年運用

障害ゼロ

基盤化した結果
* 工数削減
* 品質担保
* UX向上

今後
* DLのURLの毎回取得の改善
* Unityの進化に追従する





## アナザーエデンにおける非同期オートセーブを用いた通信待ちストレスのないゲーム体験の実現

### なぜオートセーブ
テーマ

コンシューマゲームっぽい作り方をする

ユーザーデータ : ハイブリッド


クライアント > サーバ
* バックグラウンドでのデータ同期
* データスキーマ共有
* クライアントサイドDBMS
* チート対策

クライアント > サーバとは
大きなコードベース           小さなコードベース
データ設計の主体
クライアント単体動作
開発                          イベントフック主体

クライアント LevelDb サーバDynamoDb
サーバーでデータを作らない。作ったところからどんどんサーバーに差分を送る
DropBoxのクライアントアプリで一般的



### オートセーブのデモとその仕組
最適なオートセーブとは
* いつ中断しても途中からできる
* 割り込みの多いスマホでも納得感のある挙動
=> ステートマシンによるオートセーブ制御 最後のAutoSavingが出たタイミングで再開できる

* 没入感を邪魔しない
* ネットワーク環境に依存しない
=>キューイングとバックグラウンド通信の組み合わせ

オートセーブのタイミング
* 設定UIクローズ
* エリア移動時
* luaイベント終了時
* シナリオ進捗時
* バトル終了時
* 一分に1回

State プレイヤー操作中 → 別のステートにいく→もどるタイミングでオートセーブ

オートセーブでなにを保存しているか
ユーザーデータに変更があるか
 * dirtyflagを精査
 * dirtyなレコードをローカルDBへ書き込み
 * 差分一覧をサーバー通知用に別保存  msgpack


キューイングとバックグラウンド通信の組み合わせ
クライアントの最新状態=サーバのミラー状態 + 前回の保存からの差分
これを維持することで、非同期ながらもデータの一貫性を高い制度で確保
* 何かあったらサーバーを正とする

同期
クライアントのローカルDBに
1、最新状態
2、一つ前の保存状態から変更があったレコード

差分を送信
* バックグラウンドでどんどん送信する
  * リトライしたりする。ひとつずつ順番に送る
  * 1差分1リクエスト
  * Ackがなければ差分を送り続ける
* 不整合が起きたらサーバーデータにロールバック

差分は100個まで貯めれる
* 100超えたらタイトルに戻る
* 10分間失敗し続けたら戻る

残ってたらタイトルで掃除してから起動


ブロッキング通信
* 有償通貨のガチャ
* 有償通貨を報酬にする機能

ブロッキング通信とバックグラウンド通信
1. バックグラウンド通信キューの残りを確認
1. バックグラウンド通信を全部叩く
1. 終わったらブロッキング通信



サーバからクライアントへの同期
* ガチャ
* 補填
* データの補正

しかし非同期の通信と競合

サーバからクライアントへのデータ更新命令
全レスポンスに専用の領域がある
OperationBuilder
1. 変更する命令送信
1. クライアントがデータ変更
1. オートセーブ時に差分をサーバに送信
1. サーバ命令削除

サーバ
* クライアントから完了報告くるまでは処理がおわっていない
* 終わるまで何度もおくる
クライアント
* 処理完了を必ず差分と一緒に保存する
  * 二回実行を防ぐ
  * 2回実行してもいいが実行前の状態に巻き戻してから


### LevelDB DynamoDB FlatBuffers
クライアントで動くDBMSの選択
KVSならある LevelDB => サーバ Dynamo

google oss

dynamoDB
スケールアウトできるかがクリティカル

flatbuffers
DSLとして利用 独自拡張

マスタデータ
  暗号化を独自実装している
ユーザーデータ
  FlatBuffersでSchemaかいてmsgpackかjsonでIOする



### 運用の実際
クリア 40~60時間
LevelDB 1MB
DynamoDB Partition 128くらい

サーバ障害でのサービス停止は4回
   DynamoDBのキャパシティ管理のオペミス 3
   Redisおちた 1

DynamoDB運用
キャパシティ設定のスケジューリング
オートスケーリング
スロットル


### まとめ
* Firebase Cognitoの独自実装版

* FlatBuffers  schemaをきちんと定義している
* スケールアウトするDBをつかってる
* チート対策をオートセーブに組み込み


* 何も考えずにすべての通信をブロッキング通信にしてませんか
   * 適切に制御する
   * ゲームデザインあってこそではある
反省
* データ壊れたときの修復がスマートでなかった
  * データを2系統でもっておくなどすべきだった
  * 遠隔で補正できるようにはしてあるので手でできる



QA
チート対策は?
=> tokenをサーバーから入れてクライアントが署名して返す、ログは追える形にしている。判断はする必要がある

OperationBuilderのチート対策
=> サーバーから送ってクライアントがコマンドが署名して返してサーバーが検証。事後、バッチなどで確認したりする

100件ためる意味は
=> メモリの問題
